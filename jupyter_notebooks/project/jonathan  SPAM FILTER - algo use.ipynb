{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "471fe467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import pymongo\n",
    "import langdetect\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import RegexpStemmer\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences \n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bdde34",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "## We fetch data from our DB:\n",
    "- list of cities\n",
    "- pre trained spam filter params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "874c1553",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = pymongo.MongoClient(\"mongodb+srv://admin:admin@clusteria.tvj6u.mongodb.net/myFirstDatabase?retryWrites=true&w=majority\")\n",
    "db = client['iadb']\n",
    "collection = db['spamfilterParams']\n",
    "\n",
    "\n",
    "p_word_given_ham = collection.find_one({'_id': \"p_word_given_ham\" })\n",
    "p_word_given_spam = collection.find_one({'_id': \"p_word_given_spam\" })\n",
    "parameters_spam = collection.find_one({'_id': \"parameters_spam\" })\n",
    "parameters_ham = collection.find_one({'_id': \"parameters_ham\" })\n",
    "p_ham = collection.find_one({'_id': \"p_ham\" })\n",
    "p_spam = collection.find_one({'_id': \"p_spam\" })\n",
    "\n",
    "\n",
    "collection = db['cities']\n",
    "cursor = collection.find({})\n",
    "fields = ['stop_name']\n",
    "cityList = pd.DataFrame(list(cursor), columns = fields)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c432df53",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "## The final spam filter algo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "f137224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classify(message, p_word_given_ham, p_word_given_spam, parameters_spam, parameters_ham, p_spam, p_ham):\n",
    "     \n",
    "    \"\"\"\n",
    "    IN : model params, user input\n",
    "    OUT : model s prediction (ham / spam)\n",
    "    USE : func that predicts weather a user input is spam or ham by\n",
    "          applying our model params to a Naive Bayes model\n",
    "          also checks for FR lang, and number of cities in the input\n",
    "          Used for predicting user input.\n",
    "    \"\"\"\n",
    "    \n",
    "    def detect_lang(text):\n",
    "        \"\"\"\n",
    "        IN: string\n",
    "        OUT: string\n",
    "        USE: returns the lang code (ex: 'fr') from the best predicted language\n",
    "        \"\"\"\n",
    "        result = langdetect.detect_langs(text)\n",
    "        lang = str(result[0])[:2]\n",
    "        return lang\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def check_two_cities(message, cityList):\n",
    "        \"\"\"\n",
    "        IN: string, list of cities extracted from our stop_names\n",
    "        OUT: int\n",
    "        USE: returns number of cities from the string that correspond to a stop_name\n",
    "        \"\"\"\n",
    "        doc = nlp(message) #lower\n",
    "\n",
    "        def saveAllCitiesInArray():\n",
    "            cities = []\n",
    "            for city in doc.ents:\n",
    "                cities.append(city.text)\n",
    "            return cities\n",
    "        cityArr = saveAllCitiesInArray()\n",
    "\n",
    "        def checkCity(city):\n",
    "            city = city.lower()\n",
    "            city = city.replace(\"-\", \" \")\n",
    "            city = city.replace(\"saint\", \"st\")\n",
    "            result = 0\n",
    "            for index, row in cityList.iterrows():\n",
    "                processedStopName = row['stop_name'].replace(\"-\", \" \").lower()\n",
    "                if (city in processedStopName):\n",
    "                    result = 1\n",
    "                    break\n",
    "                else:\n",
    "                    result = 0\n",
    "            return result\n",
    "\n",
    "\n",
    "        nbCitiesConfirmed = 0\n",
    "        for c in cityArr:\n",
    "            nbCitiesConfirmed = nbCitiesConfirmed + checkCity(c)\n",
    "\n",
    "        return (nbCitiesConfirmed)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def preprocess_string(string):\n",
    "        \"\"\"\n",
    "        IN : user input\n",
    "        OUT : cleaned user input\n",
    "        USE : will set all to lowercase, remove punctuation and stopwords,\n",
    "              remove trailing and double spaces\n",
    "        \"\"\"\n",
    "        # set all to lowercase\n",
    "        string = string.lower()\n",
    "        # remove punct\n",
    "        string = string.replace('[^\\w\\s]',' ')\n",
    "        # remove stop words\n",
    "        stop = stopwords.words('french')\n",
    "        string = ' '.join([word for word in string.split(\" \") if word not in stopwords.words('french')])\n",
    "        # replace double space by single space\n",
    "        string = string.replace('  ',' ')\n",
    "        # strip spaces\n",
    "        string = string.strip()\n",
    "        return string\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    result = \"\"\n",
    "    # check cities\n",
    "    nb_of_cities = check_two_cities(message, cityList)\n",
    "   \n",
    "    # check lang\n",
    "    lang = detect_lang(message)\n",
    "    \n",
    "    message = message.replace(',','')\n",
    "    message = message.replace('-','')\n",
    "    message = message.replace(' -','')\n",
    "    message = message.replace(' /','')\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = preprocess_string(message)\n",
    "    \n",
    "    \n",
    "    message2 = \"\"\n",
    "    doc = nlp(message)\n",
    "    for token in doc:\n",
    "        message2 = message2+\" \"+token.lemma_\n",
    "\n",
    "    if lang != 'fr' and len(doc) > 3:\n",
    "        result = 'spam'\n",
    "    else:\n",
    "        if nb_of_cities < 2:\n",
    "            result = 'spam'\n",
    "        else:\n",
    "            \n",
    "            \n",
    "            message2 = message2.lower().split()\n",
    "            \n",
    "            #print(message)\n",
    "            p_spam_given_message = p_spam\n",
    "            p_ham_given_message = p_ham\n",
    "            \n",
    "            \n",
    "\n",
    "            for word in message2:\n",
    "               if word in parameters_spam:\n",
    "                  p_spam_given_message *= parameters_spam[word]\n",
    "\n",
    "               if word in parameters_ham: \n",
    "                  p_ham_given_message *= parameters_ham[word]\n",
    "\n",
    "            if p_ham_given_message > p_spam_given_message:\n",
    "               result = 'ham'\n",
    "            elif p_ham_given_message < p_spam_given_message:\n",
    "               result = 'spam'\n",
    "            else:\n",
    "                #print(message2)\n",
    "                result = 'ham'\n",
    "               #result = 'Equal proabilities, have a human classify this!'\n",
    "    #print(p_spam_given_message)\n",
    "    #print(p_ham_given_message)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd76c78f",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "229b69a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trajet', 'havre', 'paris']\n",
      "0.5\n",
      "0.5\n",
      "ham\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#message = ('bonjour je veux un billet de paris à lyon')\n",
    "#messsage = ('tu as vu ce train à paris ?')\n",
    "#message = \"nous voudrions prendre l'avion pour Caen en partance de Paris\"\n",
    "#message = \"paris, lyon\"\n",
    "message =  \" trajet Le Havre - Paris\"\n",
    "print(classify(message,p_word_given_ham['data'], p_word_given_spam['data'], parameters_spam['data'], parameters_ham['data'], p_spam['data'], p_ham['data']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac7ea06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

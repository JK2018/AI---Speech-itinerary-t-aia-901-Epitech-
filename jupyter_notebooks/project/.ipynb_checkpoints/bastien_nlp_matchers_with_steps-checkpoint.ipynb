{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dafd465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "from spacy.tokens import Span, Doc\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.language import Language\n",
    "from slugify import slugify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c57818",
   "metadata": {},
   "source": [
    "### Load French Cities\n",
    "\n",
    "This dataset come from DataGouv https://www.data.gouv.fr/en/datasets/regions-departements-villes-et-villages-de-france-et-doutre-mer/ <br>\n",
    "As our algorithm concerns SNCF trips, we will check that the destinations are located in Metropolitan France.\n",
    "\n",
    "- Read dataset to DataFrame\n",
    "- Filter metropolitan cities, remove islands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19feca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_CITY = 'initial data/cities.csv'\n",
    "\n",
    "france_cities = pd.read_csv(PATH_CITY)\n",
    "cities = france_cities.loc[france_cities['department_code'].str.contains(\"^\\d\\d$\", case=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1730db",
   "metadata": {},
   "source": [
    "### Check Franch City\n",
    "\n",
    "- Slugify city to match `['slug']` column in dataframe\n",
    "- Slug is lowercase where dash are replaced by white space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db5d8b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_french_city(city):\n",
    "    slug = slugify(city, lowercase=True, separator=' ')\n",
    "    search = cities.loc[france_cities['slug'] == slug]\n",
    "    return True if (len(search) > 0) else False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e4f50b",
   "metadata": {},
   "source": [
    "### Matcher's dictionary\n",
    "\n",
    "Action verbs associated with the city of departure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40e8f91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_actions = ['partir','être','venir','train','trajet']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b6f10f",
   "metadata": {},
   "source": [
    "Action verbs associated with the arrival city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0edfd1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "finish_actions = ['rendre','aller','arriver']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4e84e0",
   "metadata": {},
   "source": [
    "Prefix for start. This prefix is inserted before the starting city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07e288d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_adps = ['de','depuis']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9d215f",
   "metadata": {},
   "source": [
    "Prefix for destination. This prefix is inserted before the destination city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd78302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_adps = ['a','à','au','aux','en','vers']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558e8cc3",
   "metadata": {},
   "source": [
    "### Extract city from Span\n",
    "\n",
    "Create a matcher with pattern `'ENT_TYPE':'LOC'`\n",
    "- `LOC`will extract all names that are locations : cities, countries...\n",
    "- Return city extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5452596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cities(span):\n",
    "    city_matcher = Matcher(nlp.vocab)\n",
    "    city_pattern = [[{'ENT_TYPE':'LOC', 'OP': '+'}]]\n",
    "    city_matcher.add('CITY', city_pattern)\n",
    "\n",
    "    cities = []\n",
    "    for idx, (match_id, start, end) in enumerate(city_matcher(span)):\n",
    "        text = span[start:end].text\n",
    "        # Prevent some issues with \"Lyon -\" or \"- Lyon\" that are considered acceptable.\n",
    "        if check_french_city(text) and not text.startswith('-') and not text.endswith('-'):\n",
    "            cities.append(text)\n",
    "        \n",
    "    return cities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ebc84c",
   "metadata": {},
   "source": [
    "### Create Spacy Component\n",
    "\n",
    "Component will be added as pipe to process input text. It will extract the departure and the destination.\n",
    "\n",
    "- Departure pattern and destination patterns will match each string that contains in order an action, an adposition, and a location.\n",
    "- To distinguish the city of departure and the city of arrival, the actions change and are defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81ffbec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemAdpLoc1Pattern():\n",
    "    \n",
    "    lem_adp_loc_start = []\n",
    "    for lemma in start_actions:\n",
    "        pattern = [{'LEMMA': lemma},{'POS':'ADP'},{'ENT_TYPE':'LOC'}]\n",
    "        lem_adp_loc_start.append(pattern)\n",
    "    \n",
    "    lem_adp_loc_finish = []\n",
    "    for lemma in finish_actions:\n",
    "        pattern = [{'LEMMA': lemma},{'POS':'ADP'},{'ENT_TYPE':'LOC'}]\n",
    "        lem_adp_loc_finish.append(pattern)\n",
    "    \n",
    "    return {\n",
    "        \"LEM_ADP_LOC_START\": lem_adp_loc_start,\n",
    "        \"LEM_ADP_LOC_FINISH\":  lem_adp_loc_finish\n",
    "    }\n",
    "\n",
    "def processLemAdpLoc1Pattern(matcher_index, span, pending):\n",
    "    if (matcher_index == \"LEM_ADP_LOC_START\"):\n",
    "        cities = extract_cities(span)\n",
    "        if 'start' in pending: pending = {}\n",
    "        pending['start'] = cities[0]\n",
    "\n",
    "    if (matcher_index == \"LEM_ADP_LOC_FINISH\"):\n",
    "        cities = extract_cities(span)\n",
    "        if 'finish' in pending: pending = {}\n",
    "        pending['finish'] = cities[0]\n",
    "        \n",
    "    return pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e490b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adpLoc1AdpLoc2Pattern():\n",
    "    \n",
    "    adp_loc_adp_loc_start = []\n",
    "    for adp in destination_adps:\n",
    "        pattern = [{'ENT_TYPE':'LOC'},{'LEMMA': adp, 'POS':'ADP'},{'ENT_TYPE':'LOC'}]\n",
    "        adp_loc_adp_loc_start.append(pattern)\n",
    "    \n",
    "    adp_loc_adp_loc_finish = []\n",
    "    for adp in start_adps:\n",
    "        pattern = [{'ENT_TYPE':'LOC'},{'LEMMA': adp, 'POS':'ADP'},{'ENT_TYPE':'LOC'}]\n",
    "        adp_loc_adp_loc_finish.append(pattern)\n",
    "\n",
    "    return {\n",
    "        \"ADP_LOC_ADP_LOC_START\": adp_loc_adp_loc_start,\n",
    "        \"ADP_LOC_ADP_LOC_FINISH\": adp_loc_adp_loc_finish\n",
    "    }\n",
    "\n",
    "def processAdpLoc1AdpLoc2Pattern(matcher_index, span, pending):\n",
    "    if (matcher_index == \"ADP_LOC_ADP_LOC_START\"):\n",
    "        cities = extract_cities(span)\n",
    "\n",
    "        if len(cities) == 2:\n",
    "            pending['start'] = cities[0]\n",
    "            pending['finish'] = cities[1]\n",
    "\n",
    "    if (matcher_index == \"ADP_LOC_ADP_LOC_FINISH\"):\n",
    "        cities = extract_cities(span)\n",
    "\n",
    "        if len(cities) == 2:\n",
    "            pending['start'] = cities[1]\n",
    "            pending['finish'] = cities[0]\n",
    "\n",
    "    return pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18c85b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc1Loc2Pattern():\n",
    "\n",
    "    return {\n",
    "        \"LOC_LOC\": [[{'ENT_TYPE':'LOC'},{'ORTH': '-', 'OP': '?'},{'ENT_TYPE':'LOC'}]]\n",
    "    }\n",
    "\n",
    "def processLoc1Loc2Pattern(matcher_index, span, pending):\n",
    "    if (matcher_index == \"LOC_LOC\"):\n",
    "        cities = extract_cities(span)\n",
    "\n",
    "        if len(cities) == 2:\n",
    "            pending['start'] = cities[0]\n",
    "            pending['finish'] = cities[1]\n",
    "            \n",
    "        if len(cities) == 4:\n",
    "            pending['start'] = cities[0]\n",
    "            pending['finish'] = cities[3]\n",
    "\n",
    "    return pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e707b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def specificPattern():\n",
    "\n",
    "    return {\n",
    "        \"SPECIFIC_START\": [\n",
    "            [{'LEMMA': 'départ'},{'ENT_TYPE':'LOC'}],\n",
    "            [{'LEMMA': 'début'},{'ENT_TYPE':'LOC'}]\n",
    "        ],\n",
    "        \"SPECIFIC_FINISH\": [\n",
    "            [{'LEMMA': 'arrivée'},{'ENT_TYPE':'LOC'}],\n",
    "            [{'LEMMA': 'fin'},{'ENT_TYPE':'LOC'}],\n",
    "        ],\n",
    "    }\n",
    "\n",
    "def processSpecificPattern(matcher_index, span, pending):\n",
    "    if (matcher_index == \"SPECIFIC_START\"):\n",
    "        cities = extract_cities(span)\n",
    "        if 'start' in pending: pending = {}\n",
    "        pending['start'] = cities[0]\n",
    "\n",
    "    if (matcher_index == \"SPECIFIC_FINISH\"):\n",
    "        cities = extract_cities(span)\n",
    "        if 'finish' in pending: pending = {}\n",
    "        pending['finish'] = cities[0]\n",
    "\n",
    "    return pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b669a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepsPattern():\n",
    "\n",
    "    return {\n",
    "        \"STEPS\": [\n",
    "            [{'LEMMA': 'par'},{'ENT_TYPE':'LOC', 'OP': '+'}],\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def processStepsPattern(matcher_index, span, pending):\n",
    "    if (matcher_index == \"STEPS\"):\n",
    "        cities = extract_cities(span)\n",
    "        pending['steps'] = cities\n",
    "\n",
    "    return pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a50e4f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSteps(doc):\n",
    "    steps = []\n",
    "    text = doc.text\n",
    "\n",
    "    # Get all occurences of substrings that start with 'par'\n",
    "    matches = re.finditer(\" par \", text)\n",
    "    matches_positions = [match.start() for match in matches]\n",
    "\n",
    "    for position in matches_positions:\n",
    "        # Clean string and remove white spaces\n",
    "        filtered = text[position:len(text)].strip()\n",
    "        filtered = filtered.replace(',', '')\n",
    "        filtered = filtered.replace('.', '')\n",
    "        filtered = filtered.replace('-', '')\n",
    "        labels = filtered.split(' ')\n",
    "\n",
    "        # Test all words in string to find cities\n",
    "        for idx, label in enumerate(labels):\n",
    "\n",
    "            # If string, doesn't started with 'par', it's an error and exit loop\n",
    "            if (idx == 0 and label != 'par'): \n",
    "                break\n",
    "\n",
    "            if (label == 'par'):\n",
    "                continue\n",
    "                \n",
    "            # Extract 1 city from word\n",
    "            if (check_french_city(label)):\n",
    "                steps.append(label)\n",
    "                \n",
    "            else:\n",
    "                label_index = [i for i, x in enumerate(doc.text.split(' ')) if x == label]\n",
    "                if len(label_index) > 0:\n",
    "                    span = doc[label_index[0]]\n",
    "                    # If not city was found, can be a CCONJ like 'et', 'ou'..., for an addition of cities\n",
    "                    # If none of 2, steps search is over\n",
    "                    if (span.pos_ != 'CCONJ'):\n",
    "                        break\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa5f8c3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loads patterns\n",
    "patterns = {}\n",
    "patterns.update(lemAdpLoc1Pattern())\n",
    "patterns.update(adpLoc1AdpLoc2Pattern())\n",
    "patterns.update(loc1Loc2Pattern())\n",
    "patterns.update(specificPattern())\n",
    "patterns.update(stepsPattern())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3be62a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads process patterns\n",
    "processors = [\n",
    "    processLemAdpLoc1Pattern,\n",
    "    processAdpLoc1AdpLoc2Pattern,\n",
    "    processLoc1Loc2Pattern,\n",
    "    processSpecificPattern,\n",
    "    processStepsPattern\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df653baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"extract_targets\")\n",
    "def extract_targets(doc):\n",
    "\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    for pattern in patterns:\n",
    "        matcher.add(pattern, patterns[pattern])\n",
    "    \n",
    "    targets = []\n",
    "    steps = []\n",
    "    pending = {}\n",
    "    for match_id, start, end in matcher(doc):\n",
    "        matcher_index = doc.vocab.strings[match_id]\n",
    "\n",
    "        for processor in processors:\n",
    "            pending = processor(matcher_index, doc[start:end], pending)\n",
    "\n",
    "        if (len(pending) == 2):\n",
    "            targets.append(pending)\n",
    "            pending = {}\n",
    "            \n",
    "    # Specific search for steps\n",
    "    steps = findSteps(doc)\n",
    "    \n",
    "    return (targets, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8278302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.extract_targets(doc)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('fr_core_news_lg')\n",
    "nlp.add_pipe('extract_targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "930f272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    \"Je pars de Lyon pour arriver à Toulouse.\",\n",
    "    \"Demain, j'irai à Lyon, mais aujourd'hui, je suis à Montpellier.\",\n",
    "    \"Je voudrais aller à Montpellier depuis Toulouse.\",\n",
    "    \"Je vais aller de Paris à Lyon.\",\n",
    "    \"Demain, je fais le trajet Lyon - Marseille.\",\n",
    "    \"Demain, je ferai le trajet de Paris à Marseille.\",\n",
    "    \"Je compte prendre un train depuis Lille, et avec un peu de chance si la SNCF n'est pas en retard, j'arriverai à Toulouse.\",\n",
    "    \"Ville de départ Toulouse et ville d'arrivée Lille.\",\n",
    "    \"Trajet Paris à Marseille.\",\n",
    "    \"Trajet Paris depuis Marseille.\",\n",
    "    \"Aller de Toulouse à Lille demain.\",\n",
    "    \"Aller a Toulouse depuis Paris demain.\",\n",
    "    \"Départ Toulouse vers Perpignan.\",\n",
    "    \"Je suis à Toulouse, je voudrais aller demain à Perpignan.\",\n",
    "    \"Trajet de Lille à Toulouse ce soir.\",\n",
    "    \"Je voudrais aller de Toulouse à Paris après demain.\",\n",
    "    \"Trajet Lyon Toulouse aujourd'hui.\",\n",
    "    \"Je voudrais me rendre à Toulouse demain, je suis à Nantes aujourd'hui.\",\n",
    "    \"Trains disponible pour aller à Marseille en venant de Lille.\",\n",
    "    \"Lille Marseille.\",\n",
    "    \"Lille en venant de Marseille.\",\n",
    "    \"Je vais de Toulouse à Marseille en passant par Perpignan Montpellier Nice.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90615957",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je pars de Lyon pour arriver à Toulouse.\n",
      "Prediction : Lyon -> Toulouse\n",
      "\n",
      "Demain, j'irai à Lyon, mais aujourd'hui, je suis à Montpellier.\n",
      "Prediction : Montpellier -> Lyon\n",
      "\n",
      "Je voudrais aller à Montpellier depuis Toulouse.\n",
      "Prediction : Toulouse -> Montpellier\n",
      "\n",
      "Je vais aller de Paris à Lyon.\n",
      "Prediction : Paris -> Lyon\n",
      "\n",
      "Demain, je fais le trajet Lyon - Marseille.\n",
      "Prediction : Lyon -> Marseille\n",
      "\n",
      "Demain, je ferai le trajet de Paris à Marseille.\n",
      "Prediction : Paris -> Marseille\n",
      "\n",
      "Je compte prendre un train depuis Lille, et avec un peu de chance si la SNCF n'est pas en retard, j'arriverai à Toulouse.\n",
      "Prediction : Lille -> Toulouse\n",
      "\n",
      "Ville de départ Toulouse et ville d'arrivée Lille.\n",
      "Prediction : Toulouse -> Lille\n",
      "\n",
      "Trajet Paris à Marseille.\n",
      "Prediction : Paris -> Marseille\n",
      "\n",
      "Trajet Paris depuis Marseille.\n",
      "Prediction : Marseille -> Paris\n",
      "\n",
      "Aller de Toulouse à Lille demain.\n",
      "Prediction : Toulouse -> Lille\n",
      "\n",
      "Aller a Toulouse depuis Paris demain.\n",
      "Prediction : Paris -> Toulouse\n",
      "\n",
      "Départ Toulouse vers Perpignan.\n",
      "Prediction : Toulouse -> Perpignan\n",
      "\n",
      "Je suis à Toulouse, je voudrais aller demain à Perpignan.\n",
      "Prediction: []\n",
      "\n",
      "Trajet de Lille à Toulouse ce soir.\n",
      "Prediction : Lille -> Toulouse\n",
      "\n",
      "Je voudrais aller de Toulouse à Paris après demain.\n",
      "Prediction : Toulouse -> Paris\n",
      "\n",
      "Trajet Lyon Toulouse aujourd'hui.\n",
      "Prediction : Lyon -> Toulouse\n",
      "\n",
      "Je voudrais me rendre à Toulouse demain, je suis à Nantes aujourd'hui.\n",
      "Prediction : Nantes -> Toulouse\n",
      "\n",
      "Trains disponible pour aller à Marseille en venant de Lille.\n",
      "Prediction : Lille -> Marseille\n",
      "\n",
      "Lille Marseille.\n",
      "Prediction : Lille -> Marseille\n",
      "\n",
      "Lille en venant de Marseille.\n",
      "Prediction: []\n",
      "\n",
      "Je vais de Toulouse à Marseille en passant par Perpignan Montpellier Nice.\n",
      "Prediction : Toulouse -> Marseille\n",
      "Etapes: ['Perpignan', 'Montpellier', 'Nice']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in datasets:\n",
    "    print(doc)\n",
    "    predictions, steps = nlp(doc)\n",
    "    \n",
    "    if (len(predictions) == 0):\n",
    "        print('Prediction: []')\n",
    "        \n",
    "    for prediction in predictions:\n",
    "        print('Prediction : {0} -> {1}'.format(prediction['start'], prediction['finish']))\n",
    "        \n",
    "    if (len(steps) > 0):\n",
    "        print('Etapes: {0}'.format(steps))\n",
    "    \n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

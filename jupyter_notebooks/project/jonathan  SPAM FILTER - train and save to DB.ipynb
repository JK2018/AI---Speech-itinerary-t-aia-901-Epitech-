{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a634d9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import walk\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import langdetect\n",
    "import json\n",
    "from sklearn.utils import shuffle\n",
    "import warnings\n",
    "from random import randrange\n",
    "import pymongo\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import RegexpStemmer\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences \n",
    "from spacy import displacy\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "DATASET_PATH = '/Users/jonathankhalifa/Desktop/T-AIA-901/BOOTSTRAP/discours/tous'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0892eaf3",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Dataset creation and preprocessing funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "13b4f4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef remove_cities(message, cityList):\\n    \\n    IN: string that needs to be processed\\n    OUT: processed string\\n    USE: removes cities that are listed in the DB from the string\\n    \\n    doc = nlp(message) #lower\\n    \\n    def saveAllCitiesInArray():\\n        cities = []\\n        for city in doc.ents:\\n            cities.append(city.text)\\n        return cities\\n    cityArr = saveAllCitiesInArray()\\n    \\n    def checkCity(city):\\n        city = city.lower()\\n        city = city.replace(\"-\", \" \")\\n        city = city.replace(\"saint\", \"st\")\\n\\n        for index, row in cityList.iterrows():\\n            processedStopName = row[\\'stop_name\\'].replace(\"-\", \" \").lower()\\n            if (city in processedStopName):\\n                message = message.replace(city, \"\")\\n                break\\n        return message\\n    \\n    for c in cityArr:\\n        message = checkCity(c)\\n        \\n    return (messsage)\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def load_initial_dataset(path):\n",
    "    \"\"\"\n",
    "    IN : path to raw text dataset on hard drive\n",
    "    OUT : dataframe\n",
    "    \"\"\"\n",
    "    discours_path = path\n",
    "    dataset = {}\n",
    "    fullDataset = {}\n",
    "    dset = []\n",
    "    filenames = next(walk(discours_path), (None, None, {}))[2] \n",
    "\n",
    "    for f in filenames:\n",
    "        ff = open(discours_path + '/' + f, 'r')\n",
    "        file_contents = ff.read()\n",
    "\n",
    "        file_contents = file_contents.replace(\"\\n\", \" \")\n",
    "\n",
    "        x = file_contents.split('.')\n",
    "        for xx in x:\n",
    "            dset.append(xx)\n",
    "        ff.close()\n",
    "    df = pd.DataFrame(dset)\n",
    "    df.rename(columns={0: 'text_input'}, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def inject_good_words(phrase):\n",
    "    \"\"\"\n",
    "    IN : 1 text row from initial dataset\n",
    "    OUT : text with a random ham/good word injected\n",
    "    USE : inject a random ham related word at a random position in the text\n",
    "    \"\"\"\n",
    "    good_words = ['billet','train','aller', 'retour', 'vouloir', 'souhaiter', 'acheter','billet', 'départ','retour']\n",
    "    \n",
    "    split_strings = phrase.split()\n",
    "    inj_pos = len(split_strings)-1\n",
    "    inj_pos = randrange(inj_pos)\n",
    "\n",
    "    rand_word = len(good_words)-1\n",
    "    rand_word = randrange(rand_word)\n",
    "    rand_word = good_words[rand_word]\n",
    "\n",
    "    split_strings.insert(inj_pos, rand_word)\n",
    "    final_string = ' '.join(split_strings)\n",
    "    return final_string\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def inject_spam_words(phrase):\n",
    "    \"\"\"\n",
    "    IN : 1 text row from initial dataset\n",
    "    OUT : text with a random spam word injected\n",
    "    USE : inject a random spam related word at a random position in the text\n",
    "    \"\"\"\n",
    "    spam_words = ['avion', 'vol', 'aeroport', 'port', 'routière', 'autoroute', 'bus', 'autocar',\n",
    "                  'autobus', 'remboursement', 'rembourser', 'bateau', 'voiture', 'pied', 'marcher', 'concert',\n",
    "                 'dinner', 'spectacle']\n",
    "    \n",
    "    split_strings = phrase.split()\n",
    "    \n",
    "    inj_pos = len(split_strings)-1\n",
    "    inj_pos = randrange(inj_pos)\n",
    "\n",
    "    rand_word = len(spam_words)-1\n",
    "    rand_word = randrange(rand_word)\n",
    "    rand_word = spam_words[rand_word]\n",
    "\n",
    "    split_strings.insert(inj_pos, rand_word)\n",
    "    final_string = ' '.join(split_strings)\n",
    "    return final_string\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_test_val_split(dfnorm, dfspam, coef_ham, coef_spam):\n",
    "    \"\"\"\n",
    "        IN : ham dataset, spam dataset\n",
    "        OUT : train dataset, test dataset, validation dataset\n",
    "        USE : Splits dataset into traditionnal 70/29/1. coef_ham/spam \n",
    "              to set the ratio of ham/spam in each split ex: 0.8 for 80%\n",
    "    \"\"\"\n",
    "    l_validation = 10\n",
    "    l_train = round((len(dfnorm)+len(dfspam))*0.7)-10\n",
    "    l_test = round((len(dfnorm)+len(dfspam))*0.3)\n",
    "    \n",
    "    val1 = dfnorm.iloc[ :int(l_validation*coef_ham)]\n",
    "    val2 = dfspam.iloc[ :int(l_validation*coef_spam)]\n",
    "    train1 =  dfnorm.iloc[int(l_validation*coef_ham):int(l_train*coef_ham)]\n",
    "    train2 =  dfspam.iloc[int(l_validation*coef_spam):int(l_train*coef_spam)]\n",
    "    test1 =  dfnorm.iloc[int(l_validation*coef_ham)+int(l_train*coef_ham):]\n",
    "    test2 =  dfspam.iloc[int(l_validation*coef_spam)+int(l_train*coef_spam):]\n",
    "    \n",
    "    # concat spam and norm\n",
    "    df_train = pd.concat([train1, train2])\n",
    "    df_test = pd.concat([test1, test2])\n",
    "    df_val = pd.concat([val1, val2])\n",
    "    \n",
    "    # shuffle rows\n",
    "    df_train = shuffle(df_train)\n",
    "    df_test = shuffle(df_test)\n",
    "    df_val = shuffle(df_val)\n",
    "    \n",
    "    #reset indexes\n",
    "    df_val.reset_index(drop=True, inplace=True)\n",
    "    df_train.reset_index(drop=True, inplace=True)\n",
    "    df_test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    result = [df_train, df_test, df_val]\n",
    "    return result\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def remove_cities_s(message):\n",
    "    \"\"\"\n",
    "    IN: string that needs to be processed\n",
    "    OUT: processed string\n",
    "    USE: removes any city/country from the string\n",
    "    \"\"\"\n",
    "    doc = nlp(message) #lower\n",
    "    for city in doc.ents:\n",
    "        message = message.replace(str(city), \"\")\n",
    "    return (message)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def remove_cities(message, cityList):\n",
    "    \"\"\"\"\"\"\n",
    "    IN: string that needs to be processed\n",
    "    OUT: processed string\n",
    "    USE: removes cities that are listed in the DB from the string\n",
    "    \"\"\"\"\"\"\n",
    "    doc = nlp(message) #lower\n",
    "    \n",
    "    def saveAllCitiesInArray():\n",
    "        cities = []\n",
    "        for city in doc.ents:\n",
    "            cities.append(city.text)\n",
    "        return cities\n",
    "    cityArr = saveAllCitiesInArray()\n",
    "    \n",
    "    def checkCity(city):\n",
    "        city = city.lower()\n",
    "        city = city.replace(\"-\", \" \")\n",
    "        city = city.replace(\"saint\", \"st\")\n",
    "\n",
    "        for index, row in cityList.iterrows():\n",
    "            processedStopName = row['stop_name'].replace(\"-\", \" \").lower()\n",
    "            if (city in processedStopName):\n",
    "                message = message.replace(city, \"\")\n",
    "                break\n",
    "        return message\n",
    "    \n",
    "    for c in cityArr:\n",
    "        message = checkCity(c)\n",
    "        \n",
    "    return (messsage)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eb6b27",
   "metadata": {},
   "source": [
    "## Func to generate our dataset will do the following : \n",
    "    \n",
    "    Remove all texts under 100 chars long\n",
    "    Split the initial dataset in half, 50% will become spam and 50% ham\n",
    "    We remove any city name from our texts. we do not want to vectorize cities since we will later do a city check\n",
    "    We label our dataset\n",
    "    We have 50% spam and 50% ham\n",
    "    We add good words to our ham texts and bad words to our spam texts\n",
    "    We overload each spam text with bad words in order to make the bad words weigh more as spam than the good words weigh as ham.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "29ad9fd6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_labeled_dataset(df):\n",
    "    \"\"\"\n",
    "    IN : initial text dataset\n",
    "    OUT : 3 dataframes (train 70%, test 30%, val 10rows) \n",
    "    USE : Used to create the final dataset that will be used for the spam \n",
    "          filter training/testing. Dataset is splitted, labelled norm or spam\n",
    "          and injected with words according to label.\n",
    "          Coef_ham/spam to set the ratio of ham/spam ex: 0.8 for 80%\n",
    "    \"\"\"\n",
    "    \n",
    "    coef_ham = 0.5\n",
    "    coef_spam = 0.5\n",
    "    \n",
    "    \n",
    "    # remove texts with under 100 chars in length\n",
    "    for index, row in df.iterrows():\n",
    "        if len(row['text_input']) < 100:\n",
    "            df = df.drop([index])\n",
    "            \n",
    "            \n",
    "    df['text_input'] = df['text_input'].transform(remove_cities_s)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # split in half\n",
    "    a = round(len(df)*coef_ham)\n",
    "    dfnorm = df.iloc[ :a+5]\n",
    "    dfspam = df.iloc[ a-5:]\n",
    "    \n",
    "    dfnorm['text_input'] = dfnorm['text_input'].transform(inject_good_words)\n",
    "    dfnorm['text_input'] = dfnorm['text_input'].transform(inject_good_words)\n",
    "    dfnorm['text_label'] = \"ham\"\n",
    "    \n",
    "    #inj bad words\n",
    "    dfspam['text_input'] = dfspam['text_input'].transform(inject_spam_words)\n",
    "    dfspam['text_input'] = dfspam['text_input'].transform(inject_spam_words)\n",
    "    dfspam['text_input'] = dfspam['text_input'].transform(inject_spam_words)\n",
    "    dfspam['text_input'] = dfspam['text_input'].transform(inject_spam_words)\n",
    "    dfspam['text_input'] = dfspam['text_input'].transform(inject_spam_words)\n",
    "    dfspam['text_input'] = dfspam['text_input'].transform(inject_spam_words)\n",
    "    dfspam['text_input'] = dfspam['text_input'].transform(inject_spam_words)\n",
    "    dfspam['text_input'] = dfspam['text_input'].transform(inject_spam_words)\n",
    "    dfspam['text_input'] = dfspam['text_input'].transform(inject_spam_words)\n",
    "    dfspam['text_input'] = dfspam['text_input'].transform(inject_spam_words)\n",
    "    dfspam['text_input'] = dfspam['text_input'].transform(inject_spam_words)\n",
    "    dfspam['text_input'] = dfspam['text_input'].transform(inject_spam_words)\n",
    "    dfspam['text_label'] = \"spam\"\n",
    "    \n",
    "    result = train_test_val_split(dfnorm, dfspam, coef_ham, coef_spam)\n",
    "    \n",
    "\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292d31d7",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "## As result of our dataset generation, we obtain a fully labelled and splited dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e7cc375",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_initial_dataset(DATASET_PATH)\n",
    "\n",
    "df_train, df_test, df_validate = generate_labeled_dataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9017e8f8",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Funcs for preprocessing user input\n",
    "\n",
    "#### In this phase, we will do the following to each text :\n",
    "\n",
    "    set to lowercase\n",
    "    remove punctuation\n",
    "    remove special chars\n",
    "    remove stop words\n",
    "    remove extra spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4def7dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocess for before vectorizing/training\n",
    "\n",
    "def preprocess_string(string):\n",
    "    \"\"\"\n",
    "    IN : user input\n",
    "    OUT : cleaned user input\n",
    "    USE : will set all to lowercase, remove punctuation and stopwords,\n",
    "          remove trailing and double spaces\n",
    "    \"\"\"\n",
    "    # set all to lowercase\n",
    "    string = string.lower()\n",
    "    # remove punct\n",
    "    string = string.replace('[^\\w\\s]',' ')\n",
    "    # remove stop words\n",
    "    stop = stopwords.words('french')\n",
    "    string = ' '.join([word for word in string.split(\" \") if word not in stopwords.words('french')])\n",
    "    # replace double space by single space\n",
    "    string = string.replace('  ',' ')\n",
    "    # strip spaces\n",
    "    string = string.strip()\n",
    "    return string\n",
    "\n",
    "    \n",
    "def preprocess_df(df):\n",
    "    \"\"\"\n",
    "    IN : df of user inputs\n",
    "    OUT : cleaned df of user inputs\n",
    "    USE : will set all to lowercase, remove punctuation and stopwords,\n",
    "          remove trailing and double spaces\n",
    "    \"\"\"\n",
    "    df['text_input'] = df['text_input'].str.lower()\n",
    "    df['text_input'] = df['text_input'].str.replace('[^\\w\\s]',' ')\n",
    "    stop = stopwords.words('french')\n",
    "    df['text_input'] = df['text_input'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    df['text_input'] = df['text_input'].str.replace('  ',' ')\n",
    "    df['text_input'] = df['text_input'].str.strip()\n",
    "    return df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4422ce90",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## We define our training model and it's related funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b3b6883b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vocab(df_train):\n",
    "    \"\"\"\n",
    "    IN : train datasset\n",
    "    OUT : list of each unique word in the dataset\n",
    "    \"\"\"\n",
    "    df_train['text_input'] = df_train['text_input'].str.split()\n",
    "\n",
    "    vocabulary = []\n",
    "    for text in df_train['text_input']:\n",
    "        for word in text:\n",
    "            vocabulary.append(word)\n",
    "\n",
    "    vocabulary = list(set(vocabulary))\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def word_frequency(vocabulary, df_train):\n",
    "    \"\"\"\n",
    "    IN : train dataset, vocab list\n",
    "    OUT : dataset with word frequency matrix\n",
    "    \"\"\"\n",
    "    word_counts_per_text = {unique_word: [0] * len(df_train['text_input']) for unique_word in vocabulary}\n",
    "\n",
    "    for index, text in enumerate(df_train['text_input']):\n",
    "        for word in text:\n",
    "            word_counts_per_text[word][index] += 1\n",
    "\n",
    "    word_counts = pd.DataFrame(word_counts_per_text)\n",
    "    training_set_clean = pd.concat([df_train, word_counts], axis=1)\n",
    "    return training_set_clean\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calc_constants(training_set_clean, vocabulary):\n",
    "    \"\"\"\n",
    "    IN : train word frequency dataset , vocab list\n",
    "    OUT : list of model constants\n",
    "    \"\"\"\n",
    "    # Isolating spam and ham messages first\n",
    "    spam_messages = training_set_clean[training_set_clean['text_label'] == 'spam']\n",
    "    ham_messages = training_set_clean[training_set_clean['text_label'] == 'ham']\n",
    "\n",
    "    # P(Spam) and P(Ham)\n",
    "    p_spam = len(spam_messages) / len(training_set_clean)\n",
    "    p_ham = len(ham_messages) / len(training_set_clean)\n",
    "\n",
    "    # N_Spam\n",
    "    n_words_per_spam_message = spam_messages['text_input'].apply(len)\n",
    "    n_spam = n_words_per_spam_message.sum()\n",
    "\n",
    "    # N_Ham\n",
    "    n_words_per_ham_message = ham_messages['text_input'].apply(len)\n",
    "    n_ham = n_words_per_ham_message.sum()\n",
    "\n",
    "    # N_Vocabulary\n",
    "    n_vocabulary = len(vocabulary)\n",
    "\n",
    "    # Laplace smoothing\n",
    "    alpha = 1\n",
    "    return [spam_messages,ham_messages, p_spam, p_ham, n_words_per_spam_message, n_spam, n_words_per_ham_message, n_ham, n_vocabulary, alpha]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(df_train):\n",
    "    \"\"\"\n",
    "    IN : train dataset\n",
    "    OUT : list of model params\n",
    "    USE : Func used to fully train the model and retrieve the weights/params\n",
    "          To be runned once.\n",
    "    \"\"\"\n",
    "    \n",
    "    # we preprocess/clean each dataset\n",
    "    df_train = preprocess_df(df_train)\n",
    "    \n",
    "    # we extract df_train s vocabulary\n",
    "    vocabulary = extract_vocab(df_train)\n",
    "    \n",
    "    # we count the freq of each word from vocabulary in df_train\n",
    "    training_set_clean = word_frequency(vocabulary, df_train)\n",
    "    \n",
    "    # we calculate our constants\n",
    "    spam_messages, ham_messages, p_spam, p_ham, n_words_per_spam_message, n_spam, n_words_per_ham_message, n_ham, n_vocabulary, alpha = calc_constants(training_set_clean, vocabulary)\n",
    "    \n",
    "    # Initiate parameters\n",
    "    parameters_spam = {unique_word:0 for unique_word in vocabulary}\n",
    "    parameters_ham = {unique_word:0 for unique_word in vocabulary}\n",
    "\n",
    "    # Calculate parameters\n",
    "    for word in vocabulary:\n",
    "        n_word_given_spam = spam_messages[word].sum() # spam_messages already defined\n",
    "        p_word_given_spam = (n_word_given_spam + alpha) / (n_spam + alpha*n_vocabulary)\n",
    "        parameters_spam[word] = p_word_given_spam\n",
    "        \n",
    "        #print(ham_messages[word].sum())\n",
    "        n_word_given_ham = ham_messages[word].sum() # ham_messages already defined\n",
    "        p_word_given_ham = (n_word_given_ham + alpha) / (n_ham + alpha*n_vocabulary)\n",
    "        parameters_ham[word] = p_word_given_ham\n",
    "    return [p_word_given_ham, p_word_given_spam, parameters_spam, parameters_ham, p_spam, p_ham]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99994667",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "## As result of training we get the params of our model (the weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7ec934f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p_word_given_ham, p_word_given_spam, parameters_spam, parameters_ham, p_spam, p_ham = train_model(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c33170",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "## We insert the into our cloud DB:\n",
    "- The model's params\n",
    "- The city list corresponding to our our stop_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "546de0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# inssert items in mongo atlas DB\\nclient = pymongo.MongoClient(\"mongodb+srv://admin:admin@clusteria.tvj6u.mongodb.net/myFirstDatabase?retryWrites=true&w=majority\")\\ndb = client[\\'iadb\\']\\ndataset = db[\\'spamfilterParams\\']\\ndataset2 = db[\\'cities\\']\\ncollection3 = db[\\'stopNames\\']\\n\\n# from stop_names make a list of cities and save to DB\\ncursor = collection3.find({})   \\nfields = [\\'stop_name\\']\\ncityList = pd.DataFrame(list(cursor), columns = fields)\\ncityList[\\'stop_name\\']= cityList[\\'stop_name\\'].str.replace(\"Gare de \",\"\")\\n\\n\\n# to load data into mongodb\\na = {\\'_id\\': \\'p_word_given_ham\\', \\'data\\': p_word_given_ham}   \\nb = {\\'_id\\': \\'p_word_given_spam\\', \\'data\\': p_word_given_spam}\\nc = {\\'_id\\': \\'parameters_spam\\', \\'data\\': parameters_spam}\\nd = {\\'_id\\': \\'parameters_ham\\', \\'data\\': parameters_ham}\\ne = {\\'_id\\': \\'p_ham\\', \\'data\\': p_ham}\\nf = {\\'_id\\': \\'p_spam\\', \\'data\\': p_spam}\\n\\n\\n# to load city data into mongodb\\ncityList.index = cityList.index.map(str)\\ng=[]\\nfor index, row in cityList.iterrows():\\n    g.append(row.to_dict())  \\nx = dataset2.insert_many(g)\\n\\nx = dataset.insert_many([a,b,c,d,e,f])\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LAUNCH ONLY ONCE! (DONE)\n",
    "\"\"\"\n",
    "# inssert items in mongo atlas DB\n",
    "client = pymongo.MongoClient(\"mongodb+srv://admin:admin@clusteria.tvj6u.mongodb.net/myFirstDatabase?retryWrites=true&w=majority\")\n",
    "db = client['iadb']\n",
    "dataset = db['spamfilterParams']\n",
    "dataset2 = db['cities']\n",
    "collection3 = db['stopNames']\n",
    "\n",
    "# from stop_names make a list of cities and save to DB\n",
    "cursor = collection3.find({})   \n",
    "fields = ['stop_name']\n",
    "cityList = pd.DataFrame(list(cursor), columns = fields)\n",
    "cityList['stop_name']= cityList['stop_name'].str.replace(\"Gare de \",\"\")\n",
    "\n",
    "\n",
    "# to load data into mongodb\n",
    "a = {'_id': 'p_word_given_ham', 'data': p_word_given_ham}   \n",
    "b = {'_id': 'p_word_given_spam', 'data': p_word_given_spam}\n",
    "c = {'_id': 'parameters_spam', 'data': parameters_spam}\n",
    "d = {'_id': 'parameters_ham', 'data': parameters_ham}\n",
    "e = {'_id': 'p_ham', 'data': p_ham}\n",
    "f = {'_id': 'p_spam', 'data': p_spam}\n",
    "\n",
    "\n",
    "# to load city data into mongodb\n",
    "cityList.index = cityList.index.map(str)\n",
    "g=[]\n",
    "for index, row in cityList.iterrows():\n",
    "    g.append(row.to_dict())  \n",
    "x = dataset2.insert_many(g)\n",
    "\n",
    "x = dataset.insert_many([a,b,c,d,e,f])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e4a2b7",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "## We will now test and run our spam filter onto the test dataset in order to calc itss accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "becc7133",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "client = pymongo.MongoClient(\"mongodb+srv://admin:admin@clusteria.tvj6u.mongodb.net/myFirstDatabase?retryWrites=true&w=majority\")\n",
    "db = client['iadb']\n",
    "collection = db['spamfilterParams']\n",
    "\n",
    "\n",
    "p_word_given_ham = collection.find_one({'_id': \"p_word_given_ham\" })\n",
    "p_word_given_spam = collection.find_one({'_id': \"p_word_given_spam\" })\n",
    "parameters_spam = collection.find_one({'_id': \"parameters_spam\" })\n",
    "parameters_ham = collection.find_one({'_id': \"parameters_ham\" })\n",
    "p_ham = collection.find_one({'_id': \"p_ham\" })\n",
    "p_spam = collection.find_one({'_id': \"p_spam\" })\n",
    "\n",
    "\n",
    "collection = db['cities']\n",
    "cursor = collection.find({})\n",
    "fields = ['stop_name']\n",
    "cityList = pd.DataFrame(list(cursor), columns = fields)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def classify(message, p_word_given_ham, p_word_given_spam, parameters_spam, parameters_ham, p_spam, p_ham):\n",
    "     \n",
    "    \"\"\"\n",
    "    IN : model params, user input\n",
    "    OUT : model s prediction (ham / spam)\n",
    "    USE : func that predicts weather a user input is spam or ham by\n",
    "          applying our model params to a Naive Bayes model\n",
    "          also checks for FR lang, and number of cities in the input\n",
    "          Used for predicting user input.\n",
    "    \"\"\"\n",
    "    \n",
    "    def detect_lang(text):\n",
    "        \"\"\"\n",
    "        IN: string\n",
    "        OUT: string\n",
    "        USE: returns the lang code (ex: 'fr') from the best predicted language\n",
    "        \"\"\"\n",
    "        result = langdetect.detect_langs(text)\n",
    "        lang = str(result[0])[:2]\n",
    "        return lang\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def check_two_cities(message, cityList):\n",
    "        \"\"\"\n",
    "        IN: string, list of cities extracted from our stop_names\n",
    "        OUT: int\n",
    "        USE: returns number of cities from the string that correspond to a stop_name\n",
    "        \"\"\"\n",
    "        doc = nlp(message) #lower\n",
    "\n",
    "        def saveAllCitiesInArray():\n",
    "            cities = []\n",
    "            for city in doc.ents:\n",
    "                cities.append(city.text)\n",
    "            return cities\n",
    "        cityArr = saveAllCitiesInArray()\n",
    "\n",
    "        def checkCity(city):\n",
    "            city = city.lower()\n",
    "            city = city.replace(\"-\", \" \")\n",
    "            city = city.replace(\"saint\", \"st\")\n",
    "            result = 0\n",
    "            for index, row in cityList.iterrows():\n",
    "                processedStopName = row['stop_name'].replace(\"-\", \" \").lower()\n",
    "                if (city in processedStopName):\n",
    "                    result = 1\n",
    "                    break\n",
    "                else:\n",
    "                    result = 0\n",
    "            return result\n",
    "\n",
    "\n",
    "        nbCitiesConfirmed = 0\n",
    "        for c in cityArr:\n",
    "            nbCitiesConfirmed = nbCitiesConfirmed + checkCity(c)\n",
    "\n",
    "        return (nbCitiesConfirmed)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def preprocess_string(string):\n",
    "        \"\"\"\n",
    "        IN : user input\n",
    "        OUT : cleaned user input\n",
    "        USE : will set all to lowercase, remove punctuation and stopwords,\n",
    "              remove trailing and double spaces\n",
    "        \"\"\"\n",
    "        # set all to lowercase\n",
    "        string = string.lower()\n",
    "        # remove punct\n",
    "        string = string.replace('[^\\w\\s]',' ')\n",
    "        # remove stop words\n",
    "        stop = stopwords.words('french')\n",
    "        string = ' '.join([word for word in string.split(\" \") if word not in stopwords.words('french')])\n",
    "        # replace double space by single space\n",
    "        string = string.replace('  ',' ')\n",
    "        # strip spaces\n",
    "        string = string.strip()\n",
    "        return string\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    result = \"\"\n",
    "    # check cities\n",
    "    nb_of_cities = check_two_cities(message, cityList)\n",
    "   \n",
    "    # check lang\n",
    "    lang = detect_lang(message)\n",
    "    \n",
    "    message = message.replace(',','')\n",
    "    message = message.replace('-','')\n",
    "    message = message.replace(' -','')\n",
    "    message = message.replace(' /','')\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = preprocess_string(message)\n",
    "    \n",
    "    \n",
    "    message2 = \"\"\n",
    "    doc = nlp(message)\n",
    "    for token in doc:\n",
    "        message2 = message2+ \" \"+token.lemma_\n",
    "\n",
    "    if lang != 'fr' and len(doc) > 3:\n",
    "        result = 'spam'\n",
    "    else:\n",
    "        if nb_of_cities < 2:\n",
    "            result = 'spam'\n",
    "        else:\n",
    "            \n",
    "            \n",
    "            message2 = message2.lower().split()\n",
    "            \n",
    "            #print(message)\n",
    "            p_spam_given_message = p_spam\n",
    "            p_ham_given_message = p_ham\n",
    "            \n",
    "            \n",
    "\n",
    "            for word in message2:\n",
    "               if word in parameters_spam:\n",
    "                  p_spam_given_message *= parameters_spam[word]\n",
    "\n",
    "               if word in parameters_ham: \n",
    "                  p_ham_given_message *= parameters_ham[word]\n",
    "\n",
    "            if p_ham_given_message > p_spam_given_message:\n",
    "               result = 'ham'\n",
    "            elif p_ham_given_message < p_spam_given_message:\n",
    "               result = 'spam'\n",
    "            else:\n",
    "                result = 'ham'\n",
    "               #result = 'Equal proabilities, have a human classify this!'\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "24d13cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_input</th>\n",
       "      <th>text_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Je pied veux prendre un petit temps pour vol e...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Si demain on conteste que chez vous ce soit ch...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1139 Je vous annonce donc ce PEI, ces 400 mill...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Un billet dernier mot avant de conclure, pas p...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Boncourt Quand on ne peut pas faire retour viv...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3656</th>\n",
       "      <td>marcher , femme de bus , aeroport autoroute a ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3657</th>\n",
       "      <td>Beauté de départ ces côtes aux reliefs changea...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3658</th>\n",
       "      <td>bien, deux sur trois disent qu'ils sont prêts ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3659</th>\n",
       "      <td>routière de 2020 Cosne-sur-Loire sera une de a...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3660</th>\n",
       "      <td>assume les souhaiter bonnes billet choses, Ave...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3661 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             text_input text_label\n",
       "0     Je pied veux prendre un petit temps pour vol e...       spam\n",
       "1     Si demain on conteste que chez vous ce soit ch...        ham\n",
       "2     1139 Je vous annonce donc ce PEI, ces 400 mill...        ham\n",
       "3     Un billet dernier mot avant de conclure, pas p...        ham\n",
       "4     Boncourt Quand on ne peut pas faire retour viv...        ham\n",
       "...                                                 ...        ...\n",
       "3656  marcher , femme de bus , aeroport autoroute a ...       spam\n",
       "3657  Beauté de départ ces côtes aux reliefs changea...        ham\n",
       "3658  bien, deux sur trois disent qu'ils sont prêts ...        ham\n",
       "3659  routière de 2020 Cosne-sur-Loire sera une de a...       spam\n",
       "3660  assume les souhaiter bonnes billet choses, Ave...        ham\n",
       "\n",
       "[3661 rows x 2 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def inj_cities_test_df(cityList, df_test):\n",
    "\n",
    "\n",
    "    for index, row in df_test.iterrows():\n",
    "        \n",
    "        ext_pos1= randrange(len(cityList)-5) #random.randint(5,len(cityList)-5)\n",
    "        ext_pos2= randrange(len(cityList)-5) #random.randint(5,len(cityList)-5)\n",
    "        rand_word1 = cityList.iloc[ext_pos1]['stop_name']  \n",
    "        rand_word2 = cityList.iloc[ext_pos2]['stop_name']\n",
    "        \n",
    "        split_strings = row['text_input'].split()\n",
    "        \n",
    "        inj_pos1 = len(split_strings)-1\n",
    "        inj_pos1 = randrange(inj_pos1)\n",
    "        inj_pos2 = len(split_strings)-1\n",
    "        inj_pos2 = randrange(inj_pos2)\n",
    "\n",
    "        split_strings.insert(inj_pos1, rand_word1)\n",
    "        split_strings.insert(inj_pos2, rand_word2)\n",
    "        final_string = ' '.join(split_strings)\n",
    "        row['text_input'] = final_string\n",
    "    return df_test\n",
    "        \n",
    "        \n",
    "ddf = inj_cities_test_df(cityList, df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f3cb125a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good predictions : 3352\n",
      "Bad predictions : 309\n",
      "Accuracy (%) : 91.55968314668124\n"
     ]
    }
   ],
   "source": [
    "def calc_accuracy(ddf):\n",
    "\n",
    "    goodPred = 0\n",
    "    badPred = 0\n",
    "\n",
    "    for index, row in ddf.iterrows():\n",
    "\n",
    "        res = classify(row['text_input'],p_word_given_ham['data'], p_word_given_spam['data'], parameters_spam['data'], parameters_ham['data'], p_spam['data'], p_ham['data'])\n",
    "        if(res == row['text_label']):\n",
    "            goodPred = goodPred + 1\n",
    "        else:\n",
    "            badPred = badPred + 1\n",
    "\n",
    "\n",
    "    print('Good predictions :' ,goodPred)\n",
    "    print('Bad predictions :' ,badPred)\n",
    "    accuracy = goodPred/(int(len(ddf)))*100\n",
    "    return accuracy\n",
    "\n",
    "acc = calc_accuracy(ddf)\n",
    "print('Accuracy (%) :' ,acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b913f3a3",
   "metadata": {},
   "source": [
    "Using a ssample of 1000 never seen texts, we obtain an accuracy of aprox 92%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548bbd87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

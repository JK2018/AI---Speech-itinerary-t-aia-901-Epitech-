{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from enum import Enum\n",
    "from spacy.symbols import PROPN, NOUN, CCONJ, ADP, VERB\n",
    "import numpy as np\n",
    "from spacy import displacy\n",
    "\n",
    "\n",
    "texts = [\n",
    "(\"Je pars de Paris pour arriver à Toulouse.\", ['Paris', 'Toulouse']),\n",
    "(\"Demain, j'irai à Lyon, mais aujourd'hui, je suis à Montpellier.\", ['Lyon', 'Montpellier']),\n",
    "(\"Je voudrais aller à Montpellier depuis Toulouse.\", ['Montpellier', 'Toulouse']),\n",
    "(\"Je vais aller de Toulouse à Lyon.\", ['Toulouse', 'Lyon']),\n",
    "(\"Demain, je fais le trajet Paris - Marseille-Saint-Charles\", ['Paris', 'Marseille']),\n",
    "(\"Demain, je ferai le trajet de Paris à Marseille\", ['Paris', 'Marseille']),\n",
    "(\"Je compte prendre un train depuis Paris, et avec un peu de chance si la SNCF n'est pas en retard, j'arriverai à Toulouse\", ['Paris', 'Toulouse']),\n",
    "(\"Ville de départ Toulouse et ville d'arrivée Paris\", ['Toulouse', 'Paris']),\n",
    "(\"Trajet Paris à Marseille\", ['Paris', 'Marseille']),\n",
    "(\"Trajet Paris depuis Marseille\", ['Paris', 'Marseille']),\n",
    "(\"aller de Toulouse à Paris demain\", ['Paris', 'Paris']),\n",
    "(\"aller a Toulouse depuis Paris demain\", ['Toulouse', 'Paris']),\n",
    "(\"départ Toulouse vers Perpignan\", ['Toulouse', 'Perpignan']),\n",
    "(\"je suis à Toulouse , je voudrais aller demain à Perpignan\", ['Toulouse', 'Perpignan']),\n",
    "(\"trajet de Paris a Toulouse ce soir\", ['Paris', 'Toulouse']),\n",
    "(\"je voudrais aller de Toulouse à Paris après demain\", ['Toulouse', 'Paris']),\n",
    "(\"trajet Paris Toulouse aujourd'hui\", ['Paris', 'Toulouse']),\n",
    "(\"je voudrais me rendre a Toulouse demain , je suis à Nantes aujourd'hui\", ['Nantes', 'Toulouse']),\n",
    "(\"trains disponible pour aller à marseille en venant de Paris\", ['Paris', 'marseille']),\n",
    "(\"Paris Marseille\", []),\n",
    "(\"Paris en venant de Marseille\", ['Paris', 'Paris']),\n",
    "(\"Je veux aller à Paris après être allé à Mulhouse depuis Lyon\", [\"Lyon\", \"Mulhouse\", \"Paris\"]),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelDirection(Enum):\n",
    "    NONE = 1\n",
    "    START = 2\n",
    "    DEST = 3\n",
    "\n",
    "class RelStrength(Enum):\n",
    "    NONE = 1\n",
    "    WEAK = 2\n",
    "    STRONG = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word:\n",
    "    def __init__(self, word: str, direction: RelDirection, strength: RelStrength):\n",
    "        self.word = word\n",
    "        self.direction = direction\n",
    "        self.strength = strength\n",
    "\n",
    "class LinkedWord:\n",
    "    def __init__(self, word: str, fixedWord: str, direction: RelDirection, strength: RelStrength):\n",
    "        self.word = word\n",
    "        self.fixedWord = fixedWord\n",
    "        self.direction = direction\n",
    "        self.strength = strength\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CCONJ links: 'cc'_child\n",
    "CCONJ_Relation = [\n",
    "    # Start\n",
    "    Word(\"depuis\",     RelDirection.START, RelStrength.STRONG),\n",
    "    # Destination\n",
    "    Word(\"puis\",       RelDirection.DEST,  RelStrength.STRONG),\n",
    "    Word(\"et\",         RelDirection.DEST,  RelStrength.STRONG),\n",
    "    Word(\"enfin\",      RelDirection.DEST,  RelStrength.STRONG)\n",
    "]\n",
    "\n",
    "# NOUN links: 'nmod'_parent\n",
    "NOUN_Relation = [\n",
    "    # Start\n",
    "    Word(\"provenance\",     RelDirection.START, RelStrength.STRONG),\n",
    "    # Destination\n",
    "    Word(\"direction\",      RelDirection.DEST,  RelStrength.WEAK),\n",
    "    Word(\"destination\",    RelDirection.DEST,  RelStrength.WEAK)\n",
    "]\n",
    "\n",
    "# ADP_FIXED has the priority \n",
    "# ADP links: 'case'_child, 'dep'_parent\n",
    "ADP_FIXED_Relation = [\n",
    "    # Start\n",
    "    LinkedWord(\"à\",\"partir\",       RelDirection.START, RelStrength.STRONG),\n",
    "    LinkedWord(\"en\", \"partant\",    RelDirection.START, RelStrength.STRONG),\n",
    "    LinkedWord(\"mais\", \"aujourd'hui\", RelDirection.START, RelStrength.STRONG),\n",
    "    # Destination\n",
    "    LinkedWord(\"à\",\"destination\",  RelDirection.DEST,  RelStrength.STRONG),\n",
    "    LinkedWord(\"en\",\"direction\",   RelDirection.DEST,  RelStrength.WEAK)\n",
    "]\n",
    "ADP_Relation = [\n",
    "    # Start\n",
    "    Word(\"de\",     RelDirection.START, RelStrength.STRONG),\n",
    "    Word(\"du\",     RelDirection.START, RelStrength.STRONG),\n",
    "    Word(\"des\",    RelDirection.START, RelStrength.STRONG),\n",
    "    Word(\"depuis\", RelDirection.START, RelStrength.STRONG),\n",
    "    # Destination\n",
    "    Word(\"à\",      RelDirection.DEST,  RelStrength.WEAK),\n",
    "    Word(\"au\",     RelDirection.DEST,  RelStrength.WEAK),\n",
    "    Word(\"aux\",    RelDirection.DEST,  RelStrength.WEAK),\n",
    "    Word(\"dans\",   RelDirection.DEST,  RelStrength.WEAK),\n",
    "    Word(\"vers\",   RelDirection.DEST,  RelStrength.WEAK),\n",
    "    Word(\"en\",     RelDirection.DEST,  RelStrength.WEAK),\n",
    "    Word(\"par\",    RelDirection.DEST,  RelStrength.WEAK) # par : \"passer par Paris\"\n",
    "] \n",
    "\n",
    "# \"partir\" is ambiguous: \"partir de ...\" \"partir à ...\"\n",
    "VERB_MARK_Relation = [\n",
    "    Word(\"après\",   RelDirection.START, RelStrength.WEAK),\n",
    "    Word(\"avant\",   RelDirection.DEST, RelStrength.STRONG),\n",
    "    Word(\"de\",   RelDirection.START, RelStrength.STRONG),\n",
    "    Word('depuis', RelDirection.START, RelStrength.STRONG)\n",
    "]\n",
    "VERB_Relation = [\n",
    "    # Start\n",
    "    Word(\"décoller\",   RelDirection.START, RelStrength.WEAK),\n",
    "    Word(\"passer\",     RelDirection.START, RelStrength.STRONG),\n",
    "    Word(\"être\",       RelDirection.START, RelStrength.STRONG),\n",
    "    # Destination\n",
    "    Word(\"arriver\",    RelDirection.DEST,  RelStrength.STRONG),\n",
    "    Word(\"aller\",      RelDirection.DEST,  RelStrength.STRONG),\n",
    "    Word(\"visiter\",    RelDirection.DEST,  RelStrength.STRONG),\n",
    "    Word(\"atterrir\",   RelDirection.DEST,  RelStrength.STRONG),\n",
    "    Word(\"découvrir\",  RelDirection.DEST,  RelStrength.STRONG),\n",
    "    Word(\"voyager\",    RelDirection.DEST,  RelStrength.STRONG),\n",
    "    Word(\"rendre\",     RelDirection.DEST,  RelStrength.STRONG)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDirection(request):\n",
    "    print(f\"Request: {request}\")\n",
    "    locations = []\n",
    "    location_final = []\n",
    "    nlp = spacy.load(\"fr_core_news_sm\")\n",
    "    doc = nlp(request)\n",
    "    \n",
    "    \n",
    "    # for debuging \n",
    "    #for token in doc:\n",
    "    #    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n",
    "    #displacy.serve(doc, style=\"dep\")\n",
    "    \n",
    "    # extract locations\n",
    "    \n",
    "    for i in doc.ents:\n",
    "        if i.label_ == 'LOC' or i.label_ == 'GPE': \n",
    "            locations.append(i.text)\n",
    "    print(f\"Locations found: {locations}\")\n",
    "\n",
    "    if len(locations) <= 1:\n",
    "        print(\"Error while finding locations\")\n",
    "    else:\n",
    "        # Get token for each locations\n",
    "        tokens = np.zeros(len(locations), dtype=object)\n",
    "        \n",
    "        print(tokens)\n",
    "        \n",
    "        for i in range(len(locations)):\n",
    "            tokenFound = False\n",
    "            # Priority: PROPN\n",
    "            for token in doc:\n",
    "                if token.pos == PROPN:\n",
    "                    isUsable = True\n",
    "                    for tokenSelected in tokens:\n",
    "                        if type(tokenSelected) != int and tokenSelected == token:\n",
    "                            isUsable = False\n",
    "                    if isUsable:\n",
    "                        if token.text in locations[i]:\n",
    "                            tokens[i] = token\n",
    "                            tokenFound = True\n",
    "                            break\n",
    "\n",
    "            # Secondary: NOUN\n",
    "            if tokenFound == False:\n",
    "                for token in doc:\n",
    "                \n",
    "                    if token.pos == NOUN:\n",
    "                        isUsable = True\n",
    "                        for tokenSelected in tokens:\n",
    "                            if type(tokenSelected) != int and tokenSelected == token:\n",
    "                                isUsable = False\n",
    "                        if isUsable:\n",
    "                            if token.text in locations[i]:\n",
    "                                tokens[i] = token\n",
    "                                tokenFound = True\n",
    "                                break\n",
    "\n",
    "\n",
    "            # None\n",
    "            if tokenFound == False:\n",
    "                print(f\"Localization {locations[i]} not found\")\n",
    "                tokens[i] = None\n",
    "\n",
    "        # Remove None tokens\n",
    "        tmpTokens = tokens\n",
    "        tokens = [] \n",
    "        for token in tmpTokens: \n",
    "            if token != None : \n",
    "                tokens.append(token)\n",
    "\n",
    "        print(f\"Token: {tokens}\")\n",
    "\n",
    "        # Weight tokens to prepare ordering\n",
    "        weighedTokens = np.zeros(len(tokens), dtype=object)\n",
    "        \n",
    "        \n",
    "        for i in range(len(tokens)):\n",
    "            \n",
    "            print(f\"Token #{i+1} : {tokens[i].lemma_}\")\n",
    "            foundWeight = []\n",
    "            parent = tokens[i].head\n",
    "            # CCONJ (Mais où est donc Ornicar)\n",
    "            for child in tokens[i].children:\n",
    "                if child.pos == CCONJ:\n",
    "                    for ref in CCONJ_Relation:\n",
    "                        if ref.word == child.lemma_:\n",
    "                            print(f\"Found CCONJ: {ref.word} - {ref.strength.name} - {ref.direction.name}\")\n",
    "                            foundWeight.append(ref)\n",
    "                            break\n",
    "                    \n",
    "                    \n",
    "\n",
    "            # NOUN\n",
    "            if len(foundWeight) <= 0: # Not prioritary over CCONJ\n",
    "                if parent.pos == NOUN:\n",
    "                    for ref in NOUN_Relation:\n",
    "                        if ref.word == parent.lemma_:\n",
    "                            print(f\"Found NOUN: {ref.word} - {ref.strength.name} - {ref.direction.name}\")\n",
    "                            foundWeight.append(ref)\n",
    "                            break\n",
    "\n",
    "            # ADP_FIXED (Preposition : afin de; à moins de, venant de...)\n",
    "            if len(foundWeight) <= 0: # Not prioritary over CCONJ and NOUN\n",
    "                for child in tokens[i].children:\n",
    "                    if child.pos == ADP:\n",
    "                        for subChild in child.children:\n",
    "                            if subChild.dep_ == 'fixed':\n",
    "                                for ref in ADP_FIXED_Relation:\n",
    "                                    if ref.word == child.lemma_ and ref.fixedWord == subChild.lemma_:\n",
    "                                        print(f\"Found ADP_FIXED: {ref.word} {ref.fixedWord} - {ref.strength.name} - {ref.direction.name}\")\n",
    "                                        foundWeight.append(ref)\n",
    "                                        break\n",
    "\n",
    "                \n",
    "                    \n",
    "            # ADP (Preposition : à, de, pour , sur , dans )\n",
    "            if len(foundWeight) <= 0: # Not prioritary over CCONJ, NOUN and ADP_FIXED\n",
    "                for child in tokens[i].children:\n",
    "                    for ref in ADP_Relation:\n",
    "                        if ref.word == child.lemma_:\n",
    "                            print(f\"Found ADP: {ref.word} - {ref.strength.name} - {ref.direction.name}\")\n",
    "                            foundWeight.append(ref)\n",
    "                            break\n",
    "\n",
    "            # VERB_MARK\n",
    "            if len(foundWeight) <= 1: # Prioritary over CCONJ, NOUN and ADP_FIXED\n",
    "                if parent.pos == VERB:\n",
    "                    for child in parent.children:\n",
    "                        if child.dep_ == 'mark' and child.pos == ADP:\n",
    "                            for ref in VERB_MARK_Relation:\n",
    "                                if ref.word == child.lemma_:\n",
    "                                    print(f\"Found VERB_MARK: {ref.word} - {ref.strength.name} - {ref.direction.name}\")\n",
    "                                    foundWeight.append(ref)\n",
    "                                    break\n",
    "                \n",
    "            # VERB (Verbes ambigûs )\n",
    "            if len(foundWeight) <= 1: # Prioritary over CCONJ, NOUN, ADP_FIXED and VERB_MARK\n",
    "                for ref in VERB_Relation:\n",
    "                    if ref.word == parent.lemma_:\n",
    "                        print(f\"Found VERB: {ref.word} - {ref.strength.name} - {ref.direction.name}\")\n",
    "                        foundWeight.append(ref)\n",
    "                        break\n",
    "\n",
    "            # Default - Keep position \n",
    "            if len(foundWeight) == 0: # Fallback\n",
    "                print(f\"Using default weight\")\n",
    "                foundWeight.append(Word(\"default\", RelDirection.DEST,  RelStrength.WEAK))\n",
    "\n",
    "            \n",
    "            # Extract first strong relation\n",
    "            selectedWeight = None\n",
    "            for j in range(len(foundWeight)):\n",
    "                if foundWeight[j].strength == RelStrength.STRONG:\n",
    "                    selectedWeight = foundWeight[j]\n",
    "                    break\n",
    "            if selectedWeight is None:\n",
    "                selectedWeight = foundWeight[0]\n",
    "\n",
    "            print(f\"Using: {selectedWeight.word}\")\n",
    "            print(\"---------------\")\n",
    "            weighedTokens[i] = (tokens[i], selectedWeight)\n",
    "\n",
    "\n",
    "        # Order tokens\n",
    "        orderedTokens = []\n",
    "        # First pass for direction: START\n",
    "        cptStrongStrength = 0\n",
    "        for i in range(len(weighedTokens)):\n",
    "            token, weight = weighedTokens[i]\n",
    "            if weight.direction == RelDirection.START:\n",
    "                if weight.strength == RelStrength.STRONG:\n",
    "                    print(f\"First pass Ordered token : {RelStrength(weight.strength).name,RelDirection(weight.direction).name, token} for weighedStrengh Strong\")\n",
    "                    orderedTokens.insert(cptStrongStrength, token)\n",
    "                    cptStrongStrength = cptStrongStrength + 1\n",
    "                else:\n",
    "                    print(f\"First pass Ordered token : {RelStrength(weight.strength).name,RelDirection(weight.direction).name, token} for weighedStrengh Weak\")\n",
    "                    orderedTokens.append(token)\n",
    "\n",
    "        # Second pass for direction: DEST\n",
    "        cptStrongStrength = 0\n",
    "        for i in range(len(weighedTokens)):\n",
    "            token, weight = weighedTokens[i]\n",
    "            if weight.direction == RelDirection.DEST:\n",
    "                if weight.strength == RelStrength.STRONG:\n",
    "                    print(f\"Second pass Ordered token : {RelStrength(weight.strength).name, RelDirection(weight.direction).name, token}\")\n",
    "                    orderedTokens.append(token)\n",
    "                    cptStrongStrength = cptStrongStrength + 1\n",
    "                else:\n",
    "                    if cptStrongStrength == 0:\n",
    "                        print(f\"Second pass Ordered token : {RelStrength(weight.strength).name,RelDirection(weight.direction).name, token}\")\n",
    "                        orderedTokens.append(token)\n",
    "                    else:\n",
    "                        orderedTokens.insert(len(orderedTokens)-cptStrongStrength, token)\n",
    "                        print(f\"Second pass Ordered token : {RelStrength(weight.strength).name,RelDirection(weight.direction).name, token}\")\n",
    "                        \n",
    "        \n",
    "        \n",
    "        # Populate location_final cities list\n",
    "        for token in orderedTokens:\n",
    "            location_final.append(token.text)\n",
    "        print(f\"Final Location: {location_final}\")\n",
    "\n",
    "        \n",
    "\n",
    "        return location_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLP():\n",
    "    for index in range(len(texts)):\n",
    "        sentence, expectedResult = texts[index]\n",
    " \n",
    "        \n",
    "        result = getDirection(sentence)\n",
    "        print(f\"\\n\\n\\n===================================    # {index}    ===================================\")\n",
    "        print(f\"Request Sentence :    {sentence}\")\n",
    "        print(f\"result:    {result}\")\n",
    "        print(f\"expected: {expectedResult}\")\n",
    "        print(\"================================================================================================\\n\\n\\n\")\n",
    "NLP()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07efdcd4b820c98a756949507a4d29d7862823915ec7477944641bea022f4f62"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
